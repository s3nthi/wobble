---
import MainLayout from "../layouts/MainLayout.astro";
---

<MainLayout title="How It Works – Wobble">
    <main class="prose prose-invert max-w-none">
        <h1>How It Works</h1>

        <p>
            <strong>Wobble</strong> is powered by a reinforcement learning agent
            trained to solve Wordle-like puzzles using a simplified Q-learning algorithm.
        </p>

        <h2>The Learning Process</h2>

        <p>
            The agent plays Wordle-style games repeatedly, learning from trial
            and error. Each game is an <em>episode</em>, and after thousands of
            episodes the model improves its ability to pick strong guesses.
        </p>

        <ul>
            <li>
                <strong>State:</strong> After each guess, the environment provides
                feedback — how many letters are correct and in the right position
                (greens), and how many letters are correct but misplaced (yellows).
            </li>
            <li>
                <strong>Actions:</strong> The agent chooses a word to guess. Over
                time it learns which guesses lead to better outcomes.
            </li>
            <li>
                <strong>Reward:</strong> The agent receives positive points for getting
                greens and yellows, a large bonus for solving the word, and a penalty
                if it fails within 6 tries.
            </li>
        </ul>

        <p>
            Learning is done using the Q-learning update rule, which adjusts the
            value of each state–action pair:
        </p>

        <code>
            Q[s][a] ← Q[s][a] + &alpha; * (reward + &gamma; * max(Q[s&rsquo;]) - Q[s][a])
        </code>

        <p>
            Here:
            <ul>
                <li>
                    <strong>&alpha;</strong> is the learning rate (how much new info
                    overrides old).
                </li>
                <li>
                    <strong>&gamma;</strong> is the discount factor (importance of
                    future rewards).
                </li>
                <li>
                    <strong>max(Q[s&rsquo;])</strong> is the best future value from
                    the next state.
                </li>
            </ul>
        </p>

        <h2>Training</h2>

        <p>
            The agent is trained by playing against a large set of possible
            words. Over time, it learns which strategies increase the chance of
            solving the puzzle within the allowed attempts. The results of
            training are stored in a Q-table, which the bot then uses to make
            informed decisions during play.
        </p>

        <h2>Outcome</h2>
        <p>
            After training, the agent is able to approach Wordle systematically
            — starting with informative guesses, narrowing down possibilities,
            and converging on the correct word more efficiently than random
            play.
        </p>

        <br />
        <h3>Note:</h3>
        <ul>
            <li>
                No official Wordle™ code, data, or other resources are used.
            </li>
            <li>
                All training is done locally with custom word lists and
                environments.
            </li>
        </ul>
        <br />
        <p>
            <em
                >Wordle is a trademark of The New York Times Company. This
                project is not affiliated with or endorsed by The New York Times
                Company.</em
            >
        </p>
    </main>
</MainLayout>

<style>
  li {
    margin-bottom: 15px;
    list-style-type: circle;
    margin-left: 35px;
  }
</style>